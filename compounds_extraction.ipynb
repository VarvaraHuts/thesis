{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = IWNLPWrapper(lemmatizer_path='C:/Users/1/Desktop/thesis/IWNLP.Lemmatizer_20170501.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capitalize(word):\n",
    "    if len(word) > 1:\n",
    "        word = word[0].upper() + word[1:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data: 'der sandmann' by hoffmann (german-russian)\n",
    "s = open(\"C:/Users/1/Desktop/thesis/data/de-ru Hoffmann - Der Sandmann.pbo\", \"r\", encoding=\"utf-8\")\n",
    "s = s.read()\n",
    "res_s = re.findall('s=\".+?\"', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#original sentences in german\n",
    "sentences_s = []\n",
    "for item in res_s:\n",
    "    item_new = item[3:-1]\n",
    "    sentences_s.append(item_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique german tokens: not lemmatized\n",
    "tokens_s = []\n",
    "\n",
    "for sentence in sentences_s:\n",
    "    sentence_new = sentence.translate(translator)\n",
    "    tokens = sentence_new.split(\" \")\n",
    "    for token in tokens:\n",
    "        if token not in tokens_s:\n",
    "            tokens_s.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3669\n"
     ]
    }
   ],
   "source": [
    "print (len(tokens_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique german tokens: lemmatized and capitalized\n",
    "tokens_s_lemmatized = []\n",
    "for word in tokens_s:\n",
    "    lemma = lemmatizer.lemmatize_plain(word)\n",
    "    if lemma is None:\n",
    "        tokens_s_lemmatized.append(capitalize(word))\n",
    "    else:\n",
    "        tokens_s_lemmatized.append(capitalize(lemma[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compounds from GermaNet\n",
    "file_compounds = open(\"C:/Users/1/Desktop/thesis/compounds_list.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines_compounds = file_compounds.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compounds = []\n",
    "for line in lines_compounds:\n",
    "    tokens = line.split(\" \")\n",
    "    compound = tokens[0]\n",
    "    compounds.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74151\n"
     ]
    }
   ],
   "source": [
    "print (len(compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compounds in sandmann using list of compounds\n",
    "sandmann = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if token in compounds and token not in sandmann:\n",
    "        sandmann.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print (len(sandmann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top german frequency words\n",
    "file_words = open('C:/Users/1/Desktop/thesis/wortliste.txt', 'r', encoding='utf-8')\n",
    "lines_words = file_words.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#capitalizing all words\n",
    "dictionary = []\n",
    "for line in lines_words:\n",
    "    line_new = line.split('\\t')\n",
    "    dictionary.append(capitalize(line_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29819\n"
     ]
    }
   ],
   "source": [
    "print (len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_in_dic(word):\n",
    "    if word in dictionary:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_part(word):\n",
    "    if len(word) > 1 and is_in_dic(word) is True:\n",
    "        return True\n",
    "    if (word.endswith('n') or word.endswith('e') or word.endswith('s') or word.endswith('d')) and is_in_dic(word[:-1]):\n",
    "        return True\n",
    "    if (word.endswith('en') or word.endswith('er') or word.endswith('es') or word.endswith('de')) and is_in_dic(word[:-2]):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_compound(word):\n",
    "    max_ind = len(word)\n",
    "    \n",
    "    for ind, char in enumerate(word):\n",
    "        left_compound = word[0:max_ind-ind]\n",
    "        right_compound = word[max_ind-ind:max_ind]\n",
    "        right_compound2 = word[max_ind-ind+1:max_ind]\n",
    "        connect_consonant = word[max_ind-ind:max_ind-ind+1] \n",
    "        \n",
    "        if is_part(left_compound) and len(left_compound) != len(word):\n",
    "            right_compound_upper = capitalize(right_compound)\n",
    "            if is_part(right_compound_upper):\n",
    "                return word\n",
    "            \n",
    "        if is_part(left_compound) and len(left_compound) != len(word) and (connect_consonant == 's' or connect_consonant == 'n'):\n",
    "            right_compound_upper2 = capitalize(right_compound2)\n",
    "            if is_part(right_compound_upper2):\n",
    "                return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_compound_of_three(word):\n",
    "    max_ind = len(word)\n",
    "    \n",
    "    for ind1 in range(max_ind):\n",
    "        for ind2 in range(ind1, max_ind):\n",
    "            left_part = word[:ind1]\n",
    "            middle_part = word[ind1:ind2]\n",
    "            right_part = word[ind2:]\n",
    "            \n",
    "            if is_part(left_part):\n",
    "                middle_part_upper = capitalize(middle_part)\n",
    "                right_part_upper = capitalize(right_part)\n",
    "                if is_part(middle_part_upper) and is_part(right_part_upper):\n",
    "                    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compounds made of two words in sandmann using list of top frequency german words\n",
    "sandmann2 = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if is_compound(token) and token not in sandmann2:\n",
    "        sandmann2.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compounds made of three words in sandmann using list of top frequency german words\n",
    "sandmann2_2 = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if is_compound_of_three(token) and token not in sandmann2_2:\n",
    "        sandmann2_2.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "print (len(sandmann2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print (len(sandmann2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for token in sandmann2_2:\n",
    "    if token not in sandmann2:\n",
    "        sandmann2.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#annotated compounds from sandmann\n",
    "z = open(\"C:/Users/1/Desktop/thesis/sandmann_komposits_list.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines_z = z.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_z_new = []\n",
    "for line in lines_z:\n",
    "    line_new = line.strip(\"\\n\")\n",
    "    lines_z_new.append(line_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compounds_sandmann = []\n",
    "for word in lines_z_new:\n",
    "    lemma = lemmatizer.lemmatize_plain(word)\n",
    "    if lemma is None:\n",
    "        compounds_sandmann.append(capitalize(word))\n",
    "    else:\n",
    "        compounds_sandmann.append(capitalize(lemma[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "print (len(compounds_sandmann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3253588516746411\n"
     ]
    }
   ],
   "source": [
    "#list of compounds: precision\n",
    "count = 0\n",
    "for compound in sandmann:\n",
    "    if compound in compounds_sandmann:\n",
    "        count += 1\n",
    "acc = count/len(compounds_sandmann)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.861244019138756\n"
     ]
    }
   ],
   "source": [
    "#list of top frequency words: precision\n",
    "count2= 0\n",
    "for compound in sandmann2:\n",
    "    if compound in compounds_sandmann:\n",
    "        count2 += 1\n",
    "acc2 = count2/len(compounds_sandmann)\n",
    "print (acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#both approaches\n",
    "sandmann3 = []\n",
    "for compound in sandmann:\n",
    "    if compound not in sandmann2:\n",
    "        sandmann3.append(compound)\n",
    "for compound in sandmann2:\n",
    "    sandmann3.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9138755980861244\n"
     ]
    }
   ],
   "source": [
    "#both approaches: precision\n",
    "count3= 0\n",
    "for compound in sandmann3:\n",
    "    if compound in compounds_sandmann:\n",
    "        count3 += 1\n",
    "acc3 = count3/len(compounds_sandmann)\n",
    "print (acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammenmärchen\n",
      "Doppeltgänger\n",
      "Feuerströme\n",
      "Holzpüppchen\n",
      "Kindereien\n",
      "Kleblocken\n",
      "Papierschnitzchen\n",
      "Peipendreher\n",
      "Provinzialstadt\n",
      "Schicksalspopanz\n",
      "Schosshündchen\n",
      "Schreibpults\n",
      "Stossrapieren\n",
      "Taschenperspektiv\n",
      "Herzinnigstgeliebter\n",
      "Holdlächelnden\n",
      "Weisshauptige\n",
      "Zähnfletschend\n"
     ]
    }
   ],
   "source": [
    "#words that were not recognised as compounds\n",
    "for comp in compounds_sandmann:\n",
    "    if comp not in sandmann3:\n",
    "        print (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestalt\n",
      "Verbreiten\n",
      "Dampfenden\n",
      "Innerstes\n",
      "Gestalten\n",
      "Vierzehn\n",
      "Tausend\n",
      "Holdlächelnd\n",
      "Sichtenden\n",
      "Sausend\n",
      "Innersten\n",
      "Siegmund\n",
      "Strahlenden\n",
      "Siegmunds\n",
      "Ersterben\n"
     ]
    }
   ],
   "source": [
    "#words that were wrongly recognised as compounds\n",
    "for comp in sandmann3:\n",
    "    if comp not in compounds_sandmann:\n",
    "        print (comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
