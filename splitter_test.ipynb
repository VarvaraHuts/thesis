{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from iwnlp.iwnlp_wrapper import IWNLPWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = IWNLPWrapper(lemmatizer_path='C:/Users/1/Desktop/thesis/IWNLP.Lemmatizer_20170501.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capitalize(word):\n",
    "    if len(word) > 1:\n",
    "        word = word[0].upper() + word[1:]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = lemmatizer.lemmatize_plain(word)\n",
    "    if lemma is None:\n",
    "        return capitalize(word)\n",
    "    else:\n",
    "        return capitalize(lemma[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data: 'der sandmann' by hoffmann (german-russian)\n",
    "s = open(\"C:/Users/1/Desktop/thesis/data/de-ru Hoffmann - Der Sandmann.pbo\", \"r\", encoding=\"utf-8\")\n",
    "s = s.read()\n",
    "res_s = re.findall('s=\".+?\"', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#original sentences in german\n",
    "sentences_s = []\n",
    "for item in res_s:\n",
    "    item_new = item[3:-1]\n",
    "    sentences_s.append(item_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique german tokens: not lemmatized\n",
    "tokens_s = []\n",
    "\n",
    "for sentence in sentences_s:\n",
    "    sentence_new = sentence.translate(translator)\n",
    "    tokens = sentence_new.split(\" \")\n",
    "    for token in tokens:\n",
    "        if token not in tokens_s:\n",
    "            tokens_s.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3669\n"
     ]
    }
   ],
   "source": [
    "print (len(tokens_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unique german tokens: lemmatized and capitalized\n",
    "tokens_s_lemmatized = []\n",
    "for word in tokens_s:\n",
    "    lemma = get_lemma(word)\n",
    "    tokens_s_lemmatized.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#top german frequency words\n",
    "file_words = open('C:/Users/1/Desktop/thesis/wortliste.txt', 'r', encoding='utf-8')\n",
    "lines_words = file_words.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#capitalizing all words\n",
    "dictionary = []\n",
    "for line in lines_words:\n",
    "    line_new = line.split(' ')\n",
    "    dictionary.append(capitalize(line_new[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29441\n"
     ]
    }
   ],
   "source": [
    "print (len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_in_dic(word):\n",
    "    if word in dictionary:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_part(word):\n",
    "    if len(word) > 1 and is_in_dic(word) is True:\n",
    "        return True\n",
    "    if (word.endswith('n') or word.endswith('e') or word.endswith('s')) and is_in_dic(word[:-1]):\n",
    "        return True\n",
    "    if (word.endswith('en') or word.endswith('er') or word.endswith('es')) and is_in_dic(word[:-2]):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2-component compounds\n",
    "def is_compound(word):\n",
    "    max_ind = len(word)\n",
    "    \n",
    "    for ind, char in enumerate(word):\n",
    "        left_compound = word[0:max_ind-ind]\n",
    "        right_compound = word[max_ind-ind:max_ind]\n",
    "\n",
    "        if is_part(left_compound) and len(left_compound) != len(word):\n",
    "            right_compound_upper = capitalize(right_compound)\n",
    "            if is_part(right_compound_upper):\n",
    "                return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3-component compounds\n",
    "def is_compound_of_three(word):\n",
    "    max_ind = len(word)\n",
    "    \n",
    "    for ind1 in range(max_ind):\n",
    "        for ind2 in range(ind1, max_ind):\n",
    "            left_part = word[:ind1]\n",
    "            middle_part = word[ind1:ind2]\n",
    "            right_part = word[ind2:]\n",
    "            \n",
    "            if is_part(left_part):\n",
    "                middle_part_upper = capitalize(middle_part)\n",
    "                right_part_upper = capitalize(right_part)\n",
    "                if is_part(middle_part_upper) and is_part(right_part_upper):\n",
    "                    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2-component compounds using list of top frequency german words\n",
    "sandmann = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if is_compound(token) and token not in sandmann:\n",
    "        sandmann.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sandmann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3-component compounds using list of top frequency german words\n",
    "sandmann2 = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if is_compound_of_three(token) and token not in sandmann2:\n",
    "        sandmann2.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sandmann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compounds from GermaNet\n",
    "file_compounds = open(\"C:/Users/1/Desktop/thesis/compounds_list.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines_compounds = file_compounds.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compounds = []\n",
    "for line in lines_compounds:\n",
    "    tokens = line.split(\" \")\n",
    "    compound = tokens[0]\n",
    "    compounds.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74151\n"
     ]
    }
   ],
   "source": [
    "print (len(compounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compounds using list of compounds\n",
    "sandmann3 = []\n",
    "for token in tokens_s_lemmatized:\n",
    "    if token in compounds and token not in sandmann3:\n",
    "        sandmann3.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print (len(sandmann3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#adding 3-component compounds to 2-component list\n",
    "for token in sandmann2:\n",
    "    if token not in sandmann:\n",
    "        sandmann.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#annotated compounds from sandmann\n",
    "z = open(\"C:/Users/1/Desktop/thesis/sandmann_komposits_annotated.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines_z = z.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines_z_new = []\n",
    "for line in lines_z:\n",
    "    line_new = line.strip(\"\\n\")\n",
    "    lines_z_new.append(line_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compounds_sandmann = []\n",
    "for word in lines_z_new:\n",
    "    lemma = lemmatizer.lemmatize_plain(word)\n",
    "    if lemma is None:\n",
    "        compounds_sandmann.append(capitalize(word))\n",
    "    else:\n",
    "        compounds_sandmann.append(capitalize(lemma[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "print (len(compounds_sandmann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8585365853658536\n"
     ]
    }
   ],
   "source": [
    "#list of top frequency words: precision\n",
    "count = 0\n",
    "for compound in sandmann:\n",
    "    if compound in compounds_sandmann:\n",
    "        count += 1\n",
    "acc = count/len(compounds_sandmann)\n",
    "print (acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33170731707317075\n"
     ]
    }
   ],
   "source": [
    "#list of compounds: precision\n",
    "count2 = 0\n",
    "for compound in sandmann3:\n",
    "    if compound in compounds_sandmann:\n",
    "        count2 += 1\n",
    "acc2 = count2/len(compounds_sandmann)\n",
    "print (acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#both approaches\n",
    "sandmann4 = []\n",
    "for compound in sandmann:\n",
    "    sandmann4.append(compound)\n",
    "for compound in sandmann3:\n",
    "    if compound not in sandmann:\n",
    "        sandmann4.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9170731707317074\n"
     ]
    }
   ],
   "source": [
    "#both approaches: precision\n",
    "count3= 0\n",
    "for compound in sandmann4:\n",
    "    if compound in compounds_sandmann:\n",
    "        count3 += 1\n",
    "acc3 = count3/len(compounds_sandmann)\n",
    "print (acc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ammenmärchen\n",
      "Doppeltgänger\n",
      "Feuerströme\n",
      "Holzpüppchen\n",
      "Kindereien\n",
      "Kleblocken\n",
      "Papierschnitzchen\n",
      "Peipendreher\n",
      "Provinzialstadt\n",
      "Schicksalspopanz\n",
      "Schosshündchen\n",
      "Schreibpults\n",
      "Stossrapieren\n",
      "Taschenperspektiv\n",
      "Hellblinkende\n",
      "Herzinnigstgeliebter\n",
      "Weisshauptige\n"
     ]
    }
   ],
   "source": [
    "#words that were not recognised as compounds\n",
    "for comp in compounds_sandmann:\n",
    "    if comp not in sandmann4:\n",
    "        print (comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gestalt\n",
      "Innerstes\n",
      "Gestalten\n",
      "Innersten\n",
      "Siegmund\n",
      "Siegmunds\n",
      "Ersterben\n"
     ]
    }
   ],
   "source": [
    "#words that were wrongly recognised as compounds\n",
    "for comp in sandmann4:\n",
    "    if comp not in compounds_sandmann:\n",
    "        print (comp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
